{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jena_weather_data = pd.read_csv('data/jena_climate_2009_2016.csv')\n",
    "jena_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluding date attribute\n",
    "jena_weather_data_array = jena_weather_data.iloc[:,1:].values\n",
    "\n",
    "jena_weather_data_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore data\n",
    "\n",
    "- Temperature has a periodic pattern\n",
    "- Second plot  shows temperature values for the first 10 days\n",
    "    - the weather data captures a sample for every timestep = 10 mins\n",
    "    - in a day, there'd be 6*24 = 144 timesteps\n",
    "    - for the first 10 days, there'd be 144 * 10 = 1440 timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = jena_weather_data_array[:,1]\n",
    "plt.plot(range(len(temp)),temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1440),temp[:1440]) # for first 10 days ; each timestep is for 10 mts, implying 144 timesteps a day!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem formulation\n",
    "\n",
    "* given data going as far back as lookback timesteps (a timestep is 10 minutes) and sampled every steps timesteps, can you predict the temperature in delay timesteps?\n",
    "\n",
    "    - lookback = **720** — Observations will go back ___5 days___.\n",
    "    - steps = **6** — Observations will be sampled at one data point per hour.\n",
    "    - delay = **144** — Targets will be 24 hours in the future.\n",
    " \n",
    " \n",
    "* Normalize the data as each feature/attribute is of different scale, which may not bode well for learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(array_like_data, std=None, mean=None):\n",
    "    \n",
    "    if std is None:\n",
    "        std = np.std(array_like_data, axis=0)\n",
    "    if mean is None:\n",
    "        mean = np.mean(array_like_data, axis=0)\n",
    "    \n",
    "    array_like_data -= mean\n",
    "    array_like_data /= std\n",
    "    \n",
    "    return array_like_data, std, mean\n",
    "\n",
    "\n",
    "(data, data_std, data_mean) = normalize_features(jena_weather_data_array[:20000])\n",
    "(val_data, val_data_std,val_data_mean) = normalize_features(jena_weather_data_array[280000:300000],data_std,data_mean)\n",
    "(test_data, test_data_std, test_data_mean) = normalize_features(jena_weather_data_array[380000:400000],data_std, data_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generator\n",
    "def data_generator(data, lookback, delay, min_index, max_index,shuffle=False, batch_size=128, step=6):\n",
    "    \n",
    "    if max_index is None:\n",
    "        max_index = len(data) - delay - 1\n",
    "    \n",
    "    i = min_index + lookback\n",
    "    \n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(min_index + lookback, max_index, size=batch_size)\n",
    "        else:\n",
    "            if i + batch_size >= max_index:\n",
    "                i = min_index + lookback\n",
    "            rows = np.arange(i, min(i + batch_size, max_index))\n",
    "            i += len(rows)\n",
    "            \n",
    "        samples = np.zeros((len(rows),lookback // step,data.shape[-1]))\n",
    "        targets = np.zeros((len(rows),))\n",
    "        \n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = data[indices]\n",
    "            targets[j] = data[rows[j] + delay][1]\n",
    "            \n",
    "        yield samples, targets\n",
    "\n",
    "\n",
    "# Training/Validation Generators\n",
    "\n",
    "lookback = 720\n",
    "delay = 144\n",
    "step = 6\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "train_gen = data_generator(data,lookback=lookback,delay=delay,min_index=0,max_index=None,shuffle=True,step=step,\n",
    "                      batch_size=batch_size)\n",
    "\n",
    "val_gen = data_generator(val_data,lookback=lookback,delay=delay,min_index=0,max_index=None,step=step,\n",
    "                    batch_size=batch_size)\n",
    "\n",
    "test_gen = data_generator(test_data,lookback=lookback,delay=delay,min_index=0,max_index=None,step=step,\n",
    "                     batch_size=batch_size)\n",
    "\n",
    "val_steps = (len(val_data) - lookback)\n",
    "test_steps = (len(test_data) - lookback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the common sense baseline with MAE\n",
    "\n",
    "def evaluate_naive_method():\n",
    "    \n",
    "    batch_maes = []\n",
    "    for step in range(val_steps):\n",
    "        samples, targets = next(val_gen)\n",
    "        preds = samples[:, -1, 1]\n",
    "        mae = np.mean(np.abs(preds - targets))\n",
    "        batch_maes.append(mae)\n",
    "    print(np.mean(batch_maes))\n",
    "    \n",
    "evaluate_naive_method()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model = Sequential()\n",
    "\n",
    "basic_model.add(layers.Flatten(input_shape=(lookback // step,data.shape[-1])))\n",
    "basic_model.add(layers.Dense(32,activation='relu'))\n",
    "basic_model.add(layers.Dense(1))\n",
    "\n",
    "basic_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model.compile(optimizer=RMSprop(), loss='mae')\n",
    "\n",
    "basic_model_fit = basic_model.fit_generator(train_gen, steps_per_epoch=10,epochs=1, \n",
    "                                            validation_data=val_gen,validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = basic_model_fit.history['loss']\n",
    "val_loss = basic_model_fit.history['val_loss']\n",
    "\n",
    "epochs = range(1,len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'b-',label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r-',label='Validation Loss')\n",
    "plt.title('Basic ML Model: Training and Validation Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent baseline\n",
    "\n",
    "basic_recurrent_model = Sequential()\n",
    "\n",
    "basic_recurrent_model.add(layers.GRU(32,input_shape=(None,data.shape[-1])))\n",
    "basic_recurrent_model.add(layers.Dense(1))\n",
    "\n",
    "basic_recurrent_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_recurrent_model.compile(optimizer=RMSprop(),loss='mae')\n",
    "\n",
    "basic_recurrent_model_fit = basic_recurrent_model.fit_generator(train_gen, steps_per_epoch=10, epochs=1, \n",
    "                                                                validation_data=val_gen, validation_steps=val_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = basic_recurrent_model_fit.history['loss']\n",
    "val_loss = basic_recurrent_model_fit.history['val_loss']\n",
    "\n",
    "epochs = range(1,len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'b-',label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r-',label='Validation Loss')\n",
    "plt.title('Basic Recurrent Model: Training and Validation Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Layer with dropout\n",
    "- For regularization\n",
    "- In recurrent layers, dropout has to be handled differently for input units and recurrent units, because the recurrent units have different representations at each timestep. Therefore, unless handled properly, the recurrent layer won't be able to learn properly\n",
    "- Achieved in Keras through a separate, additional argument - `recurrent_dropout` ; which could be managed separately from `dropout` parameter that caters only to the input units of the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_recurrent_model = Sequential()\n",
    "\n",
    "basic_recurrent_model.add(layers.GRU(32,input_shape=(None,data.shape[-1]), dropout=0.2, recurrent_dropout=0.5))\n",
    "basic_recurrent_model.add(layers.Dense(1))\n",
    "\n",
    "basic_recurrent_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_recurrent_model.compile(optimizer=RMSprop(),loss='mae')\n",
    "\n",
    "basic_recurrent_model_fit = basic_recurrent_model.fit_generator(train_gen, steps_per_epoch=10, epochs=1, \n",
    "                                                                validation_data=val_gen, validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = basic_recurrent_model_fit.history['loss']\n",
    "val_loss = basic_recurrent_model_fit.history['val_loss']\n",
    "\n",
    "epochs = range(1,len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'b-',label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r-',label='Validation Loss')\n",
    "plt.title('Basic Recurrent Model (with Dropout): Training and Validation Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked Recurrent layers\n",
    "\n",
    "- Stacking is computationally expensive\n",
    "- However, could work in favour if the additional representations are useful\n",
    "- One thing to ensure is that all the internal representation across each timestep of the first recurrent layer has to be returned so that it's available to the stacked recurrent layer on top of it\n",
    "- Possible in Keras using the argument `return_sequences`\n",
    "- It can be noted that in the stacked layer (second GRU), `relu` is used as `activation` and `input_shape` is not defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_recurrent_model = Sequential()\n",
    "\n",
    "stacked_recurrent_model.add(layers.GRU(32,input_shape=(None,data.shape[-1]), dropout=0.2, recurrent_dropout=0.5, \n",
    "                                       return_sequences=True))\n",
    "stacked_recurrent_model.add(layers.GRU(64,activation='relu', dropout=0.2, recurrent_dropout=0.5))\n",
    "stacked_recurrent_model.add(layers.Dense(1))\n",
    "\n",
    "stacked_recurrent_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_recurrent_model.compile(optimizer=RMSprop(),loss='mae')\n",
    "\n",
    "stacked_recurrent_model = basic_recurrent_model.fit_generator(train_gen, steps_per_epoch=10, epochs=1, \n",
    "                                                                validation_data=val_gen, validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidirectional_gru_model = Sequential()\n",
    "\n",
    "bidirectional_gru_model.add(\n",
    "    layers.Bidirectional(\n",
    "    layers.GRU(32,input_shape=(None,data.shape[-1]))\n",
    "    ))\n",
    "bidirectional_gru_model.add(layers.Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error!!! - yet to be resolved!\n",
    "\n",
    "bidirectional_gru_model.compile(optimizer=RMSprop(),loss='mae')\n",
    "# bidirectional_gru_model.summary()\n",
    "\n",
    "bidirectional_gru_model = bidirectional_gru_model.fit_generator(train_gen, steps_per_epoch=10, epochs=1, \n",
    "                                                                validation_data=val_gen, validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to try out\n",
    "* There are many other things you could try, in order to improve performance on the temperature-forecasting problem:\n",
    "    - Adjust the number of units in each recurrent layer in the stacked setup. Thecurrent choices are largely arbitrary and thus probably suboptimal.\n",
    "    - Adjust the learning rate used by the RMSprop optimizer.\n",
    "    - Try using LSTM layers instead of GRU layers.\n",
    "    - Try using a bigger densely connected regressor on top of the recurrent layers: \n",
    "        that is, a bigger Dense layer or even a stack of Dense layers.\n",
    "    - Don’t forget to eventually run the best-performing models (in terms of validation MAE ) on the test set! Otherwise, you’ll develop architectures that are overfitting to the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learnings\n",
    "\n",
    "- it’s good to first establish common-sense baselines for your metric of choice. If you don’t have a baseline to beat, you can’t tell whether you’re making real progress\n",
    "- Try simple models before expensive ones, to justify the additional expense. Sometimes a simple model will turn out to be your best option.\n",
    "- When you have data where temporal ordering matters, recurrent networks are a great fit and easily outperform models that first flatten the temporal data.\n",
    "- To use dropout with recurrent networks, you should use a time-constant drop-out mask and recurrent dropout mask. These are built into Keras recurrent layers, so all you have to do is use the dropout and recurrent_dropout arguments of recurrent layers.\n",
    "- Stacked RNN s provide more representational power than a single RNN layer. They’re also much more expensive and thus not always worth it. Although they offer clear gains on complex problems (such as machine translation), they may not always be relevant to smaller, simpler problems.\n",
    "- Bidirectional RNN s, which look at a sequence both ways, are useful on natural-language processing problems. But they aren’t strong performers on sequence data where the recent past is much more informative than the beginning of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_1Dconvnet_combination_model = Sequential()\n",
    "\n",
    "rnn_1Dconvnet_combination_model.add(layers.Conv1D(32,kernel_size=5,activation='relu',input_shape=(None, data.shape[-1])))\n",
    "rnn_1Dconvnet_combination_model.add(layers.MaxPool1D(3))\n",
    "rnn_1Dconvnet_combination_model.add(layers.Conv1D(32,5,activation='relu'))\n",
    "rnn_1Dconvnet_combination_model.add(layers.GRU(32,dropout=0.2,recurrent_dropout=0.5))\n",
    "rnn_1Dconvnet_combination_model.add(layers.Dense(1))\n",
    "\n",
    "rnn_1Dconvnet_combination_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_1Dconvnet_combination_model.compile(optimizer=RMSprop(), loss='mae')\n",
    "\n",
    "rnn_1Dconvnet_combination_model_fit = rnn_1Dconvnet_combination_model.fit_generator(train_gen,steps_per_epoch=10,epochs=2,\n",
    "                                                                                    validation_data=val_gen,\n",
    "                                                                                   validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key take aways about 1D Convnets\n",
    "\n",
    "- In the same way that 2D convnets perform well for processing visual patterns in 2D space, 1D convnets perform well for processing temporal patterns. They offer a faster alternative to RNN s on some problems, in particular natural-language processing tasks.\n",
    "- Typically, 1D convnets are structured much like their 2D equivalents from the world of computer vision: they consist of stacks of Conv1D layers and Max-Pooling1D layers, ending in a global pooling operation or flattening operation. \n",
    "- Because RNN s are extremely expensive for processing very long sequences, but 1D convnets are cheap, it can be a good idea to use a 1D convnet as a preprocessing step before an RNN , shortening the sequence and extracting useful representations for the RNN to process.\n",
    "\n",
    "## Sequence processing - Summary\n",
    "\n",
    "* In this chapter, you learned the following techniques, which are widely applicable to any dataset of sequence data, from text to timeseries:\n",
    "    * How to tokenize text\n",
    "    * What word embeddings are, and how to use them\n",
    "    * What recurrent networks are, and how to use them\n",
    "    * How to stack RNN layers and use bidirectional RNNs to build more-powerful sequence-processing models\n",
    "    * How to use 1D convnets for sequence processing\n",
    "    * How to combine 1D convnets and RNNs to process long sequences\n",
    "    \n",
    "    \n",
    "* You can use RNNs for timeseries regression (“predicting the future”), timeseries classification, anomaly detection in timeseries, and sequence labeling (such as identifying names or dates in sentences).\n",
    "\n",
    "\n",
    "* Similarly, you can use 1D convnets for machine translation (sequence-to-sequence convolutional models, like SliceNet a ), document classification, and spelling correction.\n",
    "\n",
    "\n",
    "* If global order matters in your sequence data, then it’s preferable to use a recurrent network to process it. This is typically the case for timeseries, where the recent past is likely to be more informative than the distant past.\n",
    "\n",
    "\n",
    "* If global ordering isn’t fundamentally meaningful, then 1D convnets will turn out to work at least as well and are cheaper. This is often the case for text data, where a keyword found at the beginning of a sentence is just as meaningful as a keyword found at the end."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
