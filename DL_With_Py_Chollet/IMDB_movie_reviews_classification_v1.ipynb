{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "# from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "NUM_WORDS = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "X represents the content as word indices\n",
    "y represents the label ; 0 or 1\n",
    "'''\n",
    "(X_train, y_train),(X_test, y_test) = imdb.load_data(num_words=NUM_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train sample: \n",
      "\n",
      " [1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 8255, 2, 349, 2637, 148, 605, 2, 8003, 15, 123, 125, 68, 2, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 2, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]\n",
      "\n",
      "y_train sample: \n",
      "\n",
      " 0\n",
      "\n",
      "\n",
      "Max range of indices:  9999\n"
     ]
    }
   ],
   "source": [
    "# Print sample data\n",
    "print(\"X_train sample: \\n\\n\", X_train[1])\n",
    "print(\"\\ny_train sample: \\n\\n\", y_train[1])\n",
    "max_sequence = max([max(sequence) for sequence in X_train])\n",
    "print(\"\\n\\nMax range of indices: \", max_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "reversed_word_index = dict([(v,k) for (k,v) in word_index.items()])\n",
    "\n",
    "'''\n",
    "decode_review:\n",
    "\n",
    "Expects a datapoint from X along with the reversed_word_index dictionary as args.\n",
    "The word index is offset by 3 while decoding because 0,1,2 represent padding, start of seq, unknown respectively.\n",
    "'''\n",
    "def decode_review(datapoint, reversed_word_index):\n",
    "    \n",
    "    text = ' '.join([reversed_word_index.get(i-3,'?') for i in datapoint])\n",
    "    return text\n",
    "\n",
    "for i in range(1):\n",
    "    print(decode_review(X_train[i], reversed_word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "* The input training data (as seen in `X_train` sample) is a list of integers. This can't be fed as input to the Neural Net directly. There are two ways to transform the input data before feeding it to the Neuralnet:\n",
    "\n",
    "    - One-hot encoding: Each sample would be vectorized into a <num_words> dimensional vector of 1s and 0s depending on the presence/absence of words\n",
    "    - Padding each training sample to make their lengths consistent and turning them into integer tensors\n",
    "* Following the first approach of One-hot encoded inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_vectorized sample: \n",
      "\n",
      " [0. 1. 1. ... 0. 0. 0.]\n",
      "\n",
      "\n",
      "y_train label:\t 0 \ty_train_vectorized sample:\t 0.0\n"
     ]
    }
   ],
   "source": [
    "# Vectorize sequences\n",
    "def vectorize_sequences(input_data, dimension=10000):\n",
    "    \n",
    "    vectorized_result = np.zeros((len(input_data),dimension))\n",
    "    for i,content in enumerate(input_data):\n",
    "        vectorized_result[i,content] = 1\n",
    "    return vectorized_result\n",
    "\n",
    "X_train_vec = vectorize_sequences(X_train)\n",
    "X_test_vec = vectorize_sequences(X_test)\n",
    "\n",
    "y_train_vec = np.asarray(y_train).astype('float32')\n",
    "y_test_vec = np.asarray(y_test).astype('float32')\n",
    "\n",
    "print(\"X_train_vectorized sample: \\n\\n\", X_train_vec[1])\n",
    "print(\"\\n\\ny_train label:\\t\", y_train[1],\"\\ty_train_vectorized sample:\\t\",y_train_vec[1])\n",
    "\n",
    "X_val_vec = X_train_vec[:1000]\n",
    "y_val_vec = y_train_vec[:1000]\n",
    "\n",
    "X_train_vec = X_train_vec[1000:]\n",
    "y_train_vec = y_train_vec[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=16,activation='relu',input_shape=(10000,)))\n",
    "model.add(Dense(16,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "24000/24000 [==============================] - 3s 118us/step - loss: 0.3676 - acc: 0.8505 - val_loss: 0.2836 - val_acc: 0.8810\n",
      "Epoch 2/10\n",
      "24000/24000 [==============================] - 3s 112us/step - loss: 0.2124 - acc: 0.9215 - val_loss: 0.2674 - val_acc: 0.8880\n",
      "Epoch 3/10\n",
      "24000/24000 [==============================] - 3s 115us/step - loss: 0.1678 - acc: 0.9385 - val_loss: 0.2994 - val_acc: 0.8790\n",
      "Epoch 4/10\n",
      "24000/24000 [==============================] - 3s 111us/step - loss: 0.1349 - acc: 0.9516 - val_loss: 0.3041 - val_acc: 0.8960\n",
      "Epoch 5/10\n",
      "24000/24000 [==============================] - 3s 110us/step - loss: 0.1081 - acc: 0.9622 - val_loss: 0.3543 - val_acc: 0.8840\n",
      "Epoch 6/10\n",
      "24000/24000 [==============================] - 3s 118us/step - loss: 0.0837 - acc: 0.9716 - val_loss: 0.4372 - val_acc: 0.8920\n",
      "Epoch 7/10\n",
      "24000/24000 [==============================] - 3s 113us/step - loss: 0.0623 - acc: 0.9792 - val_loss: 0.4269 - val_acc: 0.8820\n",
      "Epoch 8/10\n",
      "24000/24000 [==============================] - 3s 112us/step - loss: 0.0440 - acc: 0.9861 - val_loss: 0.5108 - val_acc: 0.8760\n",
      "Epoch 9/10\n",
      "24000/24000 [==============================] - 3s 118us/step - loss: 0.0312 - acc: 0.9913 - val_loss: 0.5667 - val_acc: 0.8790\n",
      "Epoch 10/10\n",
      "24000/24000 [==============================] - 3s 112us/step - loss: 0.0204 - acc: 0.9951 - val_loss: 0.6652 - val_acc: 0.8660\n"
     ]
    }
   ],
   "source": [
    "# Model compile and fit\n",
    "# model.compile(optimizer=Adam(lr=0.002),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "fit = model.fit(X_train_vec,y_train_vec,batch_size=BATCH_SIZE,epochs=EPOCHS,validation_data=(X_val_vec,y_val_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 1s 59us/step\n",
      "Test accuracy:\t 0.8474800000381469\n"
     ]
    }
   ],
   "source": [
    "eval = model.evaluate(X_test_vec,y_test_vec,batch_size=BATCH_SIZE,verbose=1)\n",
    "print(\"Test accuracy:\\t\",eval[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
