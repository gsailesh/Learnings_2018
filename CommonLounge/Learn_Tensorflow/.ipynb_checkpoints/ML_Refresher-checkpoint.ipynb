{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species']\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prepare_dataset\n",
    "---------------\n",
    "Downloads and creates a local copy of the dataset.\n",
    "Reads the dataset and separates the content into features & label.\n",
    "Returns the separated content.\n",
    "Carried out for both training & test sets.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def prepare_dataset(label_name=CSV_COLUMN_NAMES[-1]):\n",
    "    \n",
    "    train_path = tf.keras.utils.get_files(fname = TRAIN_URL.split('/')[-1], origin=TRAIN_URL)\n",
    "    test_path = tf.keras.utils.get_files(fname=TEST_URL.split('/')[-1], origin=TEST_URL)\n",
    "    \n",
    "    train_data = pd.read_csv(train_path, header=0, names=CSV_COLUMN_NAMES)\n",
    "    test_data = pd.read_csv(test_path, header=0, names=CSV_COLUMN_NAMES)\n",
    "    \n",
    "    train_features, train_label = train_data, train_data.pop(label_name)\n",
    "    test_features, test_label = test_data, test_data.pop(label_name)\n",
    "    \n",
    "    return (train_features, train_label),(test_features, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the prepare data\n",
    "(train_features, train_label),(test_features, test_label) = prepare_dataset()\n",
    "\n",
    "#train_features.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_feature_columns = []\n",
    "\n",
    "for column in train_features.columns.values:\n",
    "    data_feature_columns.append(tf.feature_column.numeric_column(key=column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters are specific to Iris data\n",
    "classifier = tf.estimator.DNNClassifier(feature_column=data_feature_columns, hidden_units=[10,10], n_classes = 3)\n",
    "\n",
    "classifier.train(input_fn=lambda:train_input_fn(train_features, train_label, args.batch_size), steps=args.train_steps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
