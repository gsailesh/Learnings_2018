{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]]\n",
      "X_standardized:  [[-0.90068117  1.03205722 -1.3412724  -1.31297673]\n",
      " [-1.14301691 -0.1249576  -1.3412724  -1.31297673]\n",
      " [-1.38535265  0.33784833 -1.39813811 -1.31297673]\n",
      " [-1.50652052  0.10644536 -1.2844067  -1.31297673]]\n"
     ]
    }
   ],
   "source": [
    "iris_data = load_iris()\n",
    "X = iris_data.data\n",
    "y = iris_data.target\n",
    "\n",
    "standardScaler = StandardScaler()\n",
    "X_standardized = standardScaler.fit_transform(X)\n",
    "\n",
    "print \"X: \", X[:2]\n",
    "print \"X_standardized: \", X_standardized[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_standardized,y, test_size=0.33, random_state=42)\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model.score(X_test, y_test) # returns mean accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "If you carry on going, you will eventually end up with the CV error beginning to go up again. \n",
    "This is because the larger you make k, the more smoothing takes place, \n",
    "and eventually you will smooth so much that you will get a model that under-fits the data rather than over-fitting it \n",
    "(make k big enough and the output will be constant regardless of the attribute values). \n",
    "I'd extend the plot until the CV error starts to go noticably up again, just to be sure, \n",
    "and then pick the k that minimizes the CV error. The bigger you make k the smoother the decision boundary and \n",
    "the more simple the model, so if computational expense is not an issue, \n",
    "I would go for a larger value of k than a smaller one, if the difference in their CV errors is negligible.\n",
    "\n",
    "If the CV error doesn't start to rise again, that probably means the attributes are not informative \n",
    "(at least for that distance metric) and giving constant outputs is the best that it can do.\n",
    "\n",
    "\n",
    "https://stats.stackexchange.com/questions/126051/choosing-optimal-k-for-knn\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
